{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from scipy import sparse\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import plaidml.keras\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(Put into a class later)'''\n",
    "def tfidf(X_data_train, X_data_test, stopwords):\n",
    "    '''Does TFIDF on training Data.'''\n",
    "    tfidf_obj = TfidfVectorizer(stop_words = stopwords,\n",
    "                                sublinear_tf = True,\n",
    "                                analyzer = 'word',\n",
    "                                token_pattern = r'\\w{2,}',\n",
    "                                ngram_range = (1, 3),\n",
    "                                max_features = 25000\n",
    "                               )\n",
    "    tvec_mat_train = tfidf_obj.fit_transform(X_data_train)    \n",
    "    tvec_mat_test = tfidf_obj.transform(X_data_test) \n",
    "    tfidf_feature_names = tfidf_obj.get_feature_names()  \n",
    "    \n",
    "    return tvec_mat_train, tvec_mat_test, tfidf_feature_names\n",
    "    \n",
    "def chi_squared(tvec_mat_train, tvec_mat_test, y_train, y_test, tfidf_feature_names):\n",
    "    '''Plots the words with highest chi-squared values and returns two new matricies \n",
    "    which have the highest chi-squared features.'''\n",
    "    chi2score = chi2(tvec_mat_train, y_train)[0]\n",
    "    plt.figure(figsize=(12,8))\n",
    "    wscores = zip(tfidf_feature_names, chi2score)\n",
    "    wchi2 = sorted(wscores, key=lambda x:x[1])\n",
    "    topchi2 = list(zip(*wchi2[-20:]))\n",
    "    x = range(len(topchi2[1]))\n",
    "    labels = topchi2[0]\n",
    "    plt.barh(x,topchi2[1], align='center', alpha=0.2)\n",
    "    plt.plot(topchi2[1], x, '-o', markersize=5, alpha=0.8)\n",
    "    plt.yticks(x, labels)\n",
    "    plt.xlabel('$\\chi^2$')\n",
    "    \n",
    "    kbest = SelectKBest(score_func = chi2, k = 15000) # Return top 15000 words as features.\n",
    "    tvec_mat_train_chi = kbest.fit_transform(tvec_mat_train, y_train)\n",
    "    tvec_mat_test_chi = kbest.fit_transform(tvec_mat_test, y_test)\n",
    "\n",
    "    return tvec_mat_train_chi, tvec_mat_test_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''No stop words.'''\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "tvec_mat_train, tvec_mat_test, tfidf_feature_names = tfidf(X_train['text'],\n",
    "                                                           X_test['text'], stopwords)\n",
    "\n",
    "tvec_mat_train_chi, tvec_mat_test_chi = chi_squared(tvec_mat_train, tvec_mat_test,\n",
    "                                                    y_train, y_test,\n",
    "                                                    tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Add stop words and return top 15000 words as features.'''\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(('wa', 'ha', 'depression', 'depressive', 'depressed', 'anxiety', 'anxious', \n",
    "                  'panic', 'attack','suicide', 'bipolar', 'manic', 'mania', 'hypomania', \n",
    "                  'attacks', 'lithium','mg', 'die', 'kill', 'disorder', 'episode', 'episodes', 'polar', 'bi'))\n",
    "\n",
    "tvec_mat_train, tvec_mat_test, tfidf_feature_names = tfidf(X_train['text'], X_test['text'], \n",
    "                                                           stopwords)\n",
    "\n",
    "tvec_mat_train_chi, tvec_mat_test_chi = chi_squared(tvec_mat_train, tvec_mat_test,\n",
    "                                                    y_train, y_test,\n",
    "                                                    tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tvec_mat_train_chi.shape)\n",
    "print(tvec_mat_test_chi.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T12:29:04.727034Z",
     "start_time": "2020-05-18T12:29:03.538850Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Scale the columns which need to be scaled.'''\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_std = scaler.fit_transform(X_train.loc[:, ~X_train.columns.isin(['text'])]) \n",
    "X_test_std = scaler.transform(X_test.loc[:, ~X_train.columns.isin(['text'])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T12:53:22.332234Z",
     "start_time": "2020-05-18T12:53:20.015624Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Remove columns with low chi2 score from sparse matrix and add them to other variables.'''\n",
    "num_feats = X_train.loc[:, ~X_train.columns.isin(['text'])].values\n",
    "X_train_tfidf = sparse.hstack((tvec_mat_train_chi, num_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T12:53:22.896245Z",
     "start_time": "2020-05-18T12:53:22.336574Z"
    }
   },
   "outputs": [],
   "source": [
    "num_feats = X_test.loc[:, ~X_test.columns.isin(['text'])].values\n",
    "X_test_tfidf = sparse.hstack((tvec_mat_test_chi, num_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docm(y_true, y_pred, labels=None):\n",
    "    '''Creates Document Matrix'''\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if gs.classes_ is not None:\n",
    "        cols = ['p_' + c for c in gs.classes_]\n",
    "        df = pd.DataFrame(cm, index=gs.classes_, columns=cols)\n",
    "    else:\n",
    "        cols = ['p_' + str(i) for i in range(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=gs.classes_)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Models and their grid-search parameters.'''\n",
    "########################################################################\n",
    "LR = LogisticRegression(multi_class = 'ovr', \n",
    "                        random_state = 1,\n",
    "                        n_jobs=16\n",
    "                       )\n",
    "LR_params = {'C': np.logspace(-5, 5, 5),\n",
    "             'solver': ['lbfgs', 'saga'],\n",
    "             'max_iter': [500, 1000],\n",
    "             'penalty': ['l1', 'l2']\n",
    "            }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "SVM_SGD = SGDClassifier(penalty='l2',\n",
    "                        random_state=1,\n",
    "                        learning_rate='optimal',\n",
    "                        tol=None,\n",
    "                        n_jobs=16\n",
    "                       )\n",
    "SVM_SGD_params = {'loss': ['hinge', 'squared_hinge'],\n",
    "                  'alpha': np.linspace(1e-3, 0.5, 5),\n",
    "                  'max_iter': [500, 1000],\n",
    "                  'penalty': ['l1', 'l2', 'elasticnet']\n",
    "                 }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "KNC = KNeighborsClassifier(n_jobs=16\n",
    "                          )\n",
    "KNC_params = {'algorithm': ['auto'],\n",
    "              'n_neighbors': [2, 10, 20],\n",
    "              'p': [1, 2],\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'metric': ['euclidean', 'manhattan']\n",
    "             }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "RFC = RandomForestClassifier(random_state=1,\n",
    "                             n_estimators=100,\n",
    "                             n_jobs=16\n",
    "                            )\n",
    "RFC_params = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [2, 5],\n",
    "              'ccp_alpha': np.linspace(0., 0.5, 3)\n",
    "             }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "ETC = ExtraTreesClassifier(random_state=1,\n",
    "                           n_estimators=100,\n",
    "                           n_jobs=16\n",
    "                          )\n",
    "ETC_params = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [2, 5],\n",
    "              'ccp_alpha': np.linspace(0., 0.5, 3)\n",
    "             }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state=1\n",
    "                            )\n",
    "DTC_params = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [2, 5],\n",
    "              'ccp_alpha': np.linspace(0., 0.5, 3)\n",
    "             }\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Grid Search'''\n",
    "\n",
    "models = {'LogisticRegression': LR, \n",
    "          'SGDClassifier': SVM_SGD, \n",
    "          'KNeighborsClassifier': KNC,\n",
    "          'DecisionTreeClassifier': DTC, \n",
    "          'RandomForestClassifier': RFC, \n",
    "          'ExtraTreeClassifier': ETC\n",
    "         }\n",
    "\n",
    "params = {'LogisticRegression': LR_params, \n",
    "          'SGDClassifier': SVM_SGD_params, \n",
    "          'KNeighborsClassifier': KNC_params,\n",
    "          'DecisionTreeClassifier': DTC_params, \n",
    "          'RandomForestClassifier': RFC_params, \n",
    "          'ExtraTreeClassifier': ETC_params\n",
    "         }\n",
    "\n",
    "score_list = []\n",
    "for name, model in models.items():\n",
    "    accuracy = 0\n",
    "    gs = GridSearchCV(estimator = model, \n",
    "                      param_grid = params[name], \n",
    "                      cv=5, \n",
    "                      verbose=1,\n",
    "                      n_jobs=16\n",
    "                     )\n",
    "    \n",
    "    gs.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    best_est = gs.best_estimator_\n",
    "    best_score = gs.best_score_ \n",
    "    best_params = gs.best_params_\n",
    "    \n",
    "    train_score = gs.score(X_train_tfidf, y_train)\n",
    "    test_score = gs.score(X_test_tfidf, y_test)\n",
    "    \n",
    "    test_predictions = gs.predict(X_test_tfidf)\n",
    "    \n",
    "    gs_results = pd.DataFrame(gs.cv_results_)\n",
    "    score_list.append([name, train_score, test_score, best_score]) # Append main results of best estimator.\n",
    "    \n",
    "    joblib.dump(gs, f'{name}' + '_model.jlib') # Save model.\n",
    "    gs_results.to_csv(f'{name}' + '_results', encoding='utf-8', index=False) # Export results to csv.\n",
    "         \n",
    "    # Print reports.\n",
    "    print(name)\n",
    "    print()\n",
    "    print(best_score)\n",
    "    print()\n",
    "    print(classification_report(y_test, test_predictions, target_names=gs.classes_))\n",
    "    print()\n",
    "    print(docm(y_test, test_predictions))\n",
    "    print()\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(score_list, columns=['model', 'train_score', 'test_score', 'best_score']) # Dataframe of main scores.\n",
    "df_scores\n",
    "\n",
    "df_scores.to_csv('/Users/francesco/df_scores', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:02:01.509586Z",
     "start_time": "2020-05-20T16:02:01.439831Z"
    }
   },
   "outputs": [],
   "source": [
    "d = dict(zip(df.subreddit.unique(), range(0,4)))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map(d, na_action='ignore')\n",
    "y_test = y_test.map(d, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:02:02.526442Z",
     "start_time": "2020-05-20T16:02:01.675331Z"
    }
   },
   "outputs": [],
   "source": [
    "plaidml.keras.install_backend()\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:02:02.543391Z",
     "start_time": "2020-05-20T16:02:02.540105Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T19:49:19.009060Z",
     "start_time": "2020-05-20T19:49:17.967220Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "alpha = 0.001\n",
    "\n",
    "model.add(keras.layers.Dense(256, activation='relu', input_shape=(X_train_std.shape[1], ), kernel_regularizer=keras.regularizers.l2(alpha))) \n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "# model.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "\n",
    "model.add(keras.layers.Dense(4, activation='softmax', kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.001)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:18:53.316477Z",
     "start_time": "2020-05-20T19:49:19.045325Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "history = model.fit(\n",
    "  X_train_tfidf, y_train,\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose=1, callbacks=[early_stop], batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:21.850448Z",
     "start_time": "2020-05-20T20:19:21.690175Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:30.589836Z",
     "start_time": "2020-05-20T20:19:22.970893Z"
    }
   },
   "outputs": [],
   "source": [
    "predicitions = model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:30.594650Z",
     "start_time": "2020-05-20T20:19:30.591207Z"
    }
   },
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:43.130700Z",
     "start_time": "2020-05-20T20:19:30.596837Z"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test_std, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:43.143308Z",
     "start_time": "2020-05-20T20:19:43.132210Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predicitions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:36:19.548728Z",
     "start_time": "2020-05-20T16:36:19.543758Z"
    }
   },
   "outputs": [],
   "source": [
    "def docm(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if model.classes_ is not None:\n",
    "        cols = ['p_' + c for c in model.classes_]\n",
    "        df = pd.DataFrame(cm, index=model.classes_, columns=cols)\n",
    "    else:\n",
    "        cols = ['p_' + str(i) for i in range(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=model.classes_)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:47:00.916858Z",
     "start_time": "2020-05-20T16:47:00.913164Z"
    }
   },
   "outputs": [],
   "source": [
    "inv_d = {v: k for k, v in d.items()}\n",
    "inv_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:49:06.505710Z",
     "start_time": "2020-05-20T16:49:06.495043Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test = y_test.map(inv_d, na_action='ignore')\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:04:56.868490Z",
     "start_time": "2020-05-20T17:04:56.864104Z"
    }
   },
   "outputs": [],
   "source": [
    "def docm(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if d.keys() is not None:\n",
    "        cols = [c for c in d.keys()]\n",
    "        df = pd.DataFrame(cm, index=d.keys(), columns=cols)\n",
    "    else:\n",
    "        cols = [str(i) for i in range(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=d.keys())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:23:12.923196Z",
     "start_time": "2020-05-21T13:23:12.911663Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \n",
    "    import itertools\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('BuGn')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.title('Confusion Matrix', fontsize=38, color='orange')\n",
    "    plt.ylabel('True label', fontsize=18)\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass), fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/Users/francesco/cmat_.png\", transparent=False, dpi=300)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:23:14.259054Z",
     "start_time": "2020-05-21T13:23:13.588207Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm           = np.array([[16420,  1102,   10293, 401],\n",
    "                                              [1392,  7687,  4834, 462],\n",
    "                                              [4191,  2042, 69423, 7041],\n",
    "                                              [565, 366, 17166, 13743]]), \n",
    "                      normalize    = False,\n",
    "                      target_names = ['anxiety', 'bipolar', 'depression', 'suicidewatch'],\n",
    "                      title        = \"Confusion Matrix NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:04:58.191775Z",
     "start_time": "2020-05-20T17:04:57.875756Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicitions.argmax(axis=1), target_names=d.keys()))\n",
    "print()\n",
    "print(docm(y_test, predicitions.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:20:31.138468Z",
     "start_time": "2020-05-20T17:20:25.607851Z"
    }
   },
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y_true = y_test\n",
    "y_probas = predicitions\n",
    "skplt.metrics.plot_roc(y_true, y_probas, figsize=(10, 10), text_fontsize=18)\n",
    "plt.title('ROC Curves NN', fontsize=38, color='orange')\n",
    "\n",
    "plt.savefig(\"/Users/francesco/ROC.png\", transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
