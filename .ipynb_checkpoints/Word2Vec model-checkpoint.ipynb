{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from scipy import sparse\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from adjustText import adjust_text\n",
    "\n",
    "import plaidml.keras\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:52:03.845077Z",
     "start_time": "2020-05-20T15:50:55.512918Z"
    }
   },
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:52:03.910474Z",
     "start_time": "2020-05-20T15:52:03.875071Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:52:03.928786Z",
     "start_time": "2020-05-20T15:52:03.917338Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.extend(('depression', 'depressive', 'depressed', 'anxiety', 'anxious', \n",
    "                      'suicide', 'suicidal', 'bipolar', 'bi', 'polar'))\n",
    "      \n",
    "    tokens = []\n",
    "    [tokens.append(word) for word in text.split() if word not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:52:09.760567Z",
     "start_time": "2020-05-20T15:52:03.933811Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size= 0.2, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:58:22.426313Z",
     "start_time": "2020-05-20T15:52:09.762935Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tokenized = X_train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "X_test_tokenized = X_test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:00:48.065395Z",
     "start_time": "2020-05-20T15:58:22.429318Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_word_average = word_averaging_list(wv,X_train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,X_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:00:48.070703Z",
     "start_time": "2020-05-20T16:00:48.066684Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train_word_average.shape)\n",
    "print(X_test_word_average.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tokenize_text(r['text'])\n",
    "word_averaging_list(wv,X_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T15:22:38.509662Z",
     "start_time": "2020-05-18T15:22:12.856451Z"
    }
   },
   "outputs": [],
   "source": [
    "words_array = [word for word in X_train_tokenized]\n",
    "words_flat = [item for sublist in words_array for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T16:01:30.612328Z",
     "start_time": "2020-05-18T16:01:29.465372Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "vector_list = [wv[word] for word in words_flat[0:20000] if word in wv.vocab]\n",
    "\n",
    "# Create a list of the words corresponding to these vectors\n",
    "words_filtered = [word for word in words_flat[0:20000] if word in wv.vocab]\n",
    "\n",
    "# Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "# Cast to a dict so we can turn it into a DataFrame\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "df_word_wv = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "df_word_wv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T16:01:31.460393Z",
     "start_time": "2020-05-18T16:01:31.433957Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_word_wv = df_word_wv.append(pd.DataFrame(wv['anxiety'].reshape(1,-1)))\n",
    "df_word_wv.rename(index = {0:'anxiety'}, inplace=True)\n",
    "\n",
    "df_word_wv = df_word_wv.append(pd.DataFrame(wv['depression'].reshape(1,-1)))\n",
    "df_word_wv.rename(index = {0:'depression'}, inplace=True)\n",
    "\n",
    "df_word_wv = df_word_wv.append(pd.DataFrame(wv['bipolar'].reshape(1,-1)))\n",
    "df_word_wv.rename(index = {0:'bipolar'}, inplace=True)\n",
    "\n",
    "df_word_wv = df_word_wv.append(pd.DataFrame(wv['suicide'].reshape(1,-1)))\n",
    "df_word_wv.rename(index = {0:'suicide'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T16:01:34.719705Z",
     "start_time": "2020-05-18T16:01:34.685840Z"
    }
   },
   "outputs": [],
   "source": [
    "df_word_wv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T16:02:24.140010Z",
     "start_time": "2020-05-18T16:01:47.948071Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 35)\n",
    "\n",
    "tsne_df = tsne.fit_transform(df_word_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:34:11.931162Z",
     "start_time": "2020-05-18T17:33:30.622256Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(figsize = (16, 12))\n",
    "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "# Import adjustText, initialize list of texts\n",
    "texts = []\n",
    "texts_extra = []\n",
    "\n",
    "words_to_plot = list(np.arange(0, len(df_word_wv)-4, 42))\n",
    "words_to_plot_extra = [4004, 4005, 4006, 4007]\n",
    "\n",
    "# Append words to list\n",
    "for word in words_to_plot:\n",
    "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df_word_wv.index[word], fontsize = 14, color = 'black'))\n",
    "\n",
    "for word in words_to_plot_extra:\n",
    "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df_word_wv.index[word], fontsize = 14, color = 'red'))\n",
    "\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,3),\n",
    "            arrowprops = dict(arrowstyle = \"->\", color = 'black', lw = 0.7))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:01:19.266875Z",
     "start_time": "2020-05-20T16:00:48.079421Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=X_train_word_average.shape[1]-1, random_state=1)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_word_average)\n",
    "\n",
    "print(pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:01:19.492624Z",
     "start_time": "2020-05-20T16:01:19.273100Z"
    }
   },
   "outputs": [],
   "source": [
    "#Scree plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.plot(pca.explained_variance_ratio_, lw=4, label='Explained variance ratio')\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), lw=4,\n",
    "         label='Cumulative explained variance ratio')\n",
    "plt.legend(loc=(1, 0.8))\n",
    "ax.set_title('Explained variance pct\\n', fontsize=20)\n",
    "ax.set_xlabel('Principal Component', fontsize=16)\n",
    "ax.set_ylabel('Variance Explained (%)', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:01:19.497712Z",
     "start_time": "2020-05-20T16:01:19.494765Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_var_ratios = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:01:19.504367Z",
     "start_time": "2020-05-20T16:01:19.499415Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    total_variance = 0.0\n",
    "    n_components = 0\n",
    "    \n",
    "    for explained_variance in var_ratio:\n",
    "        total_variance += explained_variance\n",
    "        n_components += 1\n",
    "\n",
    "        if total_variance >= goal_var:\n",
    "            break\n",
    "\n",
    "    return n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:01:19.510933Z",
     "start_time": "2020-05-20T16:01:19.506654Z"
    }
   },
   "outputs": [],
   "source": [
    "select_n_components(pca_var_ratios, 0.99) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:01:50.777856Z",
     "start_time": "2020-05-20T16:01:19.512951Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=select_n_components(pca_var_ratios, 0.99) , random_state=1)\n",
    "\n",
    "X_train_word_average_pca = pca.fit_transform(X_train_word_average)\n",
    "X_test_word_average_pca = pca.transform(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:01:52.292660Z",
     "start_time": "2020-05-20T16:01:50.781739Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_w2v = np.concatenate([X_train.loc[:, ~X_train.columns.isin(['text'])], X_train_word_average_pca], axis=1)\n",
    "X_test_w2v = np.concatenate([X_test.loc[:, ~X_train.columns.isin(['text'])], X_test_word_average_pca], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:02:01.392190Z",
     "start_time": "2020-05-20T16:01:52.295040Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_std = scaler.fit_transform(X_train_w2v) \n",
    "X_test_std = scaler.transform(X_test_w2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:53:29.391336Z",
     "start_time": "2020-05-20T17:53:29.386785Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train_std.shape)\n",
    "print(X_test_std.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docm(y_true, y_pred, labels=None):\n",
    "    '''Creates Document Matrix'''\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if gs.classes_ is not None:\n",
    "        cols = ['p_' + c for c in gs.classes_]\n",
    "        df = pd.DataFrame(cm, index=gs.classes_, columns=cols)\n",
    "    else:\n",
    "        cols = ['p_' + str(i) for i in range(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=gs.classes_)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Models and their grid-search parameters.'''\n",
    "########################################################################\n",
    "LR = LogisticRegression(multi_class = 'ovr', \n",
    "                        random_state = 1,\n",
    "                        n_jobs=16\n",
    "                       )\n",
    "LR_params = {'C': np.logspace(-5, 5, 5),\n",
    "             'solver': ['lbfgs', 'saga'],\n",
    "             'max_iter': [500, 1000],\n",
    "             'penalty': ['l1', 'l2']\n",
    "            }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "SVM_SGD = SGDClassifier(penalty='l2',\n",
    "                        random_state=1,\n",
    "                        learning_rate='optimal',\n",
    "                        tol=None,\n",
    "                        n_jobs=16\n",
    "                       )\n",
    "SVM_SGD_params = {'loss': ['hinge', 'squared_hinge'],\n",
    "                  'alpha': np.linspace(1e-3, 0.5, 5),\n",
    "                  'max_iter': [500, 1000],\n",
    "                  'penalty': ['l1', 'l2', 'elasticnet']\n",
    "                 }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "KNC = KNeighborsClassifier(n_jobs=16\n",
    "                          )\n",
    "KNC_params = {'algorithm': ['auto'],\n",
    "              'n_neighbors': [2, 10, 20],\n",
    "              'p': [1, 2],\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'metric': ['euclidean', 'manhattan']\n",
    "             }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "RFC = RandomForestClassifier(random_state=1,\n",
    "                             n_estimators=100,\n",
    "                             n_jobs=16\n",
    "                            )\n",
    "RFC_params = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [2, 5],\n",
    "              'ccp_alpha': np.linspace(0., 0.5, 3)\n",
    "             }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "ETC = ExtraTreesClassifier(random_state=1,\n",
    "                           n_estimators=100,\n",
    "                           n_jobs=16\n",
    "                          )\n",
    "ETC_params = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [2, 5],\n",
    "              'ccp_alpha': np.linspace(0., 0.5, 3)\n",
    "             }\n",
    "\n",
    "########################################################################\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state=1\n",
    "                            )\n",
    "DTC_params = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [2, 5],\n",
    "              'ccp_alpha': np.linspace(0., 0.5, 3)\n",
    "             }\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Grid Search'''\n",
    "\n",
    "models = {'LogisticRegression': LR, \n",
    "          'SGDClassifier': SVM_SGD, \n",
    "          'KNeighborsClassifier': KNC,\n",
    "          'DecisionTreeClassifier': DTC, \n",
    "          'RandomForestClassifier': RFC, \n",
    "          'ExtraTreeClassifier': ETC\n",
    "         }\n",
    "\n",
    "params = {'LogisticRegression': LR_params, \n",
    "          'SGDClassifier': SVM_SGD_params, \n",
    "          'KNeighborsClassifier': KNC_params,\n",
    "          'DecisionTreeClassifier': DTC_params, \n",
    "          'RandomForestClassifier': RFC_params, \n",
    "          'ExtraTreeClassifier': ETC_params\n",
    "         }\n",
    "\n",
    "score_list = []\n",
    "for name, model in models.items():\n",
    "    accuracy = 0\n",
    "    gs = GridSearchCV(estimator = model, \n",
    "                      param_grid = params[name], \n",
    "                      cv=5, \n",
    "                      verbose=1,\n",
    "                      n_jobs=16\n",
    "                     )\n",
    "    \n",
    "    gs.fit(X_train_std, y_train)\n",
    "    \n",
    "    best_est = gs.best_estimator_\n",
    "    best_score = gs.best_score_ \n",
    "    best_params = gs.best_params_\n",
    "    \n",
    "    train_score = gs.score(X_train_std, y_train)\n",
    "    test_score = gs.score(X_test_std, y_test)\n",
    "    \n",
    "    test_predictions = gs.predict(X_test_std)\n",
    "    \n",
    "    gs_results = pd.DataFrame(gs.cv_results_)\n",
    "    score_list.append([name, train_score, test_score, best_score]) # Append main results of best estimator.\n",
    "    \n",
    "    joblib.dump(gs, f'{name}' + '_model.jlib') # Save model.\n",
    "    gs_results.to_csv(f'{name}' + '_results', encoding='utf-8', index=False) # Export results to csv.\n",
    "         \n",
    "    # Print reports.\n",
    "    print(name)\n",
    "    print()\n",
    "    print(best_score)\n",
    "    print()\n",
    "    print(classification_report(y_test, test_predictions, target_names=gs.classes_))\n",
    "    print()\n",
    "    print(docm(y_test, test_predictions))\n",
    "    print()\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(score_list, columns=['model', 'train_score', 'test_score', 'best_score']) # Dataframe of main scores.\n",
    "df_scores\n",
    "\n",
    "df_scores.to_csv('/Users/francesco/df_scores', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:02:01.509586Z",
     "start_time": "2020-05-20T16:02:01.439831Z"
    }
   },
   "outputs": [],
   "source": [
    "d = dict(zip(df.subreddit.unique(), range(0,4)))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map(d, na_action='ignore')\n",
    "y_test = y_test.map(d, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:02:02.526442Z",
     "start_time": "2020-05-20T16:02:01.675331Z"
    }
   },
   "outputs": [],
   "source": [
    "plaidml.keras.install_backend()\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:02:02.543391Z",
     "start_time": "2020-05-20T16:02:02.540105Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T19:49:19.009060Z",
     "start_time": "2020-05-20T19:49:17.967220Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "alpha = 0.001\n",
    "\n",
    "model.add(keras.layers.Dense(256, activation='relu', input_shape=(X_train_std.shape[1], ), kernel_regularizer=keras.regularizers.l2(alpha))) \n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "# model.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "\n",
    "model.add(keras.layers.Dense(4, activation='softmax', kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.001)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:18:53.316477Z",
     "start_time": "2020-05-20T19:49:19.045325Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "history = model.fit(\n",
    "  X_train_std, y_train,\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose=1, callbacks=[early_stop], batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:21.850448Z",
     "start_time": "2020-05-20T20:19:21.690175Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:30.589836Z",
     "start_time": "2020-05-20T20:19:22.970893Z"
    }
   },
   "outputs": [],
   "source": [
    "predicitions = model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:30.594650Z",
     "start_time": "2020-05-20T20:19:30.591207Z"
    }
   },
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:43.130700Z",
     "start_time": "2020-05-20T20:19:30.596837Z"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test_std, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:19:43.143308Z",
     "start_time": "2020-05-20T20:19:43.132210Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predicitions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:36:19.548728Z",
     "start_time": "2020-05-20T16:36:19.543758Z"
    }
   },
   "outputs": [],
   "source": [
    "def docm(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if model.classes_ is not None:\n",
    "        cols = ['p_' + c for c in model.classes_]\n",
    "        df = pd.DataFrame(cm, index=model.classes_, columns=cols)\n",
    "    else:\n",
    "        cols = ['p_' + str(i) for i in range(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=model.classes_)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:47:00.916858Z",
     "start_time": "2020-05-20T16:47:00.913164Z"
    }
   },
   "outputs": [],
   "source": [
    "inv_d = {v: k for k, v in d.items()}\n",
    "inv_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T16:49:06.505710Z",
     "start_time": "2020-05-20T16:49:06.495043Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test = y_test.map(inv_d, na_action='ignore')\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:04:56.868490Z",
     "start_time": "2020-05-20T17:04:56.864104Z"
    }
   },
   "outputs": [],
   "source": [
    "def docm(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if d.keys() is not None:\n",
    "        cols = [c for c in d.keys()]\n",
    "        df = pd.DataFrame(cm, index=d.keys(), columns=cols)\n",
    "    else:\n",
    "        cols = [str(i) for i in range(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=d.keys())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T18:04:12.094764Z",
     "start_time": "2020-05-21T18:04:11.832622Z"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar(['depression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:23:12.923196Z",
     "start_time": "2020-05-21T13:23:12.911663Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \n",
    "    import itertools\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('BuGn')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.title('Confusion Matrix', fontsize=38, color='orange')\n",
    "    plt.ylabel('True label', fontsize=18)\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass), fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/Users/francesco/cmat_.png\", transparent=False, dpi=300)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:23:14.259054Z",
     "start_time": "2020-05-21T13:23:13.588207Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm           = np.array([[16420,  1102,   10293, 401],\n",
    "                                              [1392,  7687,  4834, 462],\n",
    "                                              [4191,  2042, 69423, 7041],\n",
    "                                              [565, 366, 17166, 13743]]), \n",
    "                      normalize    = False,\n",
    "                      target_names = ['anxiety', 'bipolar', 'depression', 'suicidewatch'],\n",
    "                      title        = \"Confusion Matrix NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:04:58.191775Z",
     "start_time": "2020-05-20T17:04:57.875756Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicitions.argmax(axis=1), target_names=d.keys()))\n",
    "print()\n",
    "print(docm(y_test, predicitions.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T17:20:31.138468Z",
     "start_time": "2020-05-20T17:20:25.607851Z"
    }
   },
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_probas = predicitions\n",
    "skplt.metrics.plot_roc(y_true, y_probas, figsize=(10, 10), text_fontsize=18)\n",
    "plt.title('ROC Curves NN', fontsize=38, color='orange')\n",
    "\n",
    "plt.savefig(\"/Users/francesco/ROC.png\", transparent=True, dpi=1000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
